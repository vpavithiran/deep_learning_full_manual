{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1450246,"sourceType":"datasetVersion","datasetId":850118}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A.","metadata":{}},{"cell_type":"code","source":"\n# Step 2: Import Libraries\nimport nltk\nfrom nltk.chat.util import Chat, reflections\n\n# Step 3: Define Rules (Predefined pairs)\npairs = [\n    (r\"my name is (.*)\", [\"Hello %1, How are you today?\"]),\n    (r\"hi|hey|hello\", [\"Hello\", \"Hey there\"]),\n    (r\"what is your name?\", [\"I am a bot created by [Your Name].\"]),\n    (r\"how are you?\", [\"I'm doing good. How about you?\"]),\n    (r\"sorry (.*)\", [\"No problem\", \"It's okay\", \"You don't need to be sorry\"]),\n    (r\"quit\", [\"Bye! Take care.\"])\n]\n\n# Step 4: Create the Chatbot\ndef chatbot():\n    print(\"Hi, I'm the chatbot you created. Type 'quit' to exit.\") \n    chat = Chat(pairs, reflections)\n    chat.converse()\n    \n# Step 5: Run the Chatbot\nif __name__ == \"__main__\":\n    chatbot()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T04:53:04.772669Z","iopub.execute_input":"2024-10-06T04:53:04.773093Z","iopub.status.idle":"2024-10-06T04:54:20.298607Z","shell.execute_reply.started":"2024-10-06T04:53:04.773053Z","shell.execute_reply":"2024-10-06T04:54:20.297109Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Hi, I'm the chatbot you created. Type 'quit' to exit.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"> my name is ramya \n"},{"name":"stdout","text":"Hello ramya , How are you today?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"> i'm good and u \n"},{"name":"stdout","text":"None\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"> how about u\n"},{"name":"stdout","text":"None\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"> quit\n"},{"name":"stdout","text":"Bye! Take care.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"B.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Step 1: Load and Preprocess the Dataset\ndef load_data(filepath):\n    try:\n        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n            lines = f.readlines()\n    except Exception as e:\n        print(f\"Error reading the file: {e}\")\n        return []\n\n    conversations = []\n    for line in lines:\n        line_parts = line.strip().split(' +++$+++ ')\n        if len(line_parts) == 5:\n            conversations.append(line_parts[4])  # Store only the dialogue part\n\n    print(f\"Loaded {len(conversations)} conversations.\")  # Debug info\n    return conversations\n\ndef create_pairs(conversations):\n    input_texts = []\n    target_texts = []\n\n    for i in range(len(conversations) - 1):\n        input_text = conversations[i]\n        target_text = conversations[i + 1]\n        target_text = '\\t' + target_text + '\\n'  # Add start and end tokens\n        input_texts.append(input_text)\n        target_texts.append(target_text)\n\n    print(f\"Created {len(input_texts)} input-target pairs.\")  # Debug info\n    return input_texts, target_texts\n\n# Load the dataset (replace with the correct path to movie_lines.txt)\nconversations = load_data('/kaggle/input/movie-dialogs/movie_lines.txt')  # Make sure this file exists\ninput_texts, target_texts = create_pairs(conversations)\n\n# Check if input_texts and target_texts are populated\nif not input_texts or not target_texts:\n    raise ValueError(\"No input or target texts were created. Please check the dataset.\")\n\n# Step 2: Tokenize and Pad the Data\n# Tokenize the input and output data\ninput_tokenizer = Tokenizer()\ntarget_tokenizer = Tokenizer()\n\ninput_tokenizer.fit_on_texts(input_texts)\ntarget_tokenizer.fit_on_texts(target_texts)\n\ninput_sequences = input_tokenizer.texts_to_sequences(input_texts)\ntarget_sequences = target_tokenizer.texts_to_sequences(target_texts)\n\n# Pad sequences to ensure uniform length\nmax_encoder_seq_length = max(len(seq) for seq in input_sequences) if input_sequences else 0\nmax_decoder_seq_length = max(len(seq) for seq in target_sequences) if target_sequences else 0\n\nencoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length, padding='post')\ndecoder_input_data = pad_sequences(target_sequences, maxlen=max_decoder_seq_length, padding='post')\n\n# Prepare decoder output data\ndecoder_output_data = np.zeros((len(target_sequences), max_decoder_seq_length, len(target_tokenizer.word_index) + 1), dtype='float32')\n\nfor i, seq in enumerate(target_sequences):\n    for t, word_idx in enumerate(seq):\n        if t > 0:\n            decoder_output_data[i, t - 1, word_idx] = 1.0\n\n# Step 3: Build the Seq2Seq Model\nnum_encoder_tokens = len(input_tokenizer.word_index) + 1\nnum_decoder_tokens = len(target_tokenizer.word_index) + 1\n\n# Encoder\nencoder_inputs = Input(shape=(None,))\nencoder_embedding = Embedding(input_dim=num_encoder_tokens, output_dim=256)(encoder_inputs)\nencoder_lstm = LSTM(256, return_state=True)\nencoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n\n# Save the encoder states to pass to the decoder\nencoder_states = [state_h, state_c]\n\n# Decoder\ndecoder_inputs = Input(shape=(None,))\ndecoder_embedding = Embedding(input_dim=num_decoder_tokens, output_dim=256)(decoder_inputs)\ndecoder_lstm = LSTM(256, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n# Step 4: Compile and Train the Model\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model (adjust epochs and batch size as needed)\nmodel.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=64, epochs=100)\n\n# Step 5: Inference Setup (for generating responses)\nencoder_model = Model(encoder_inputs, encoder_states)\n\n# Decoder setup\ndecoder_state_input_h = Input(shape=(256,))\ndecoder_state_input_c = Input(shape=(256,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\ndecoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\n\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states)\n\n# Step 6: Decode a Sequence (Generate a Response)\ndef decode_sequence(input_seq):\n    # Encode the input as state vectors\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate an empty target sequence with only the start token\n    target_seq = np.zeros((1, 1))\n    target_seq[0, 0] = target_tokenizer.word_index['\\t']\n\n    stop_condition = False\n    decoded_sentence = ''\n\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        # Sample the next token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = target_tokenizer.index_word.get(sampled_token_index, '')\n        decoded_sentence += sampled_char\n\n        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_length):\n            stop_condition = True\n\n        # Update the target sequence and states\n        target_seq = np.zeros((1, 1))\n        target_seq[0, 0] = sampled_token_index\n        states_value = [h, c]\n\n    return decoded_sentence.strip()  # Trim any extra whitespace\n\n# Step 7: Test the Chatbot\ndef chat():\n    print(\"Chatbot is ready! Type 'quit' to exit.\")\n    while True:\n        input_text = input(\"You: \")\n        if input_text.lower() == 'quit':\n            print(\"Exiting the chat. Goodbye!\")\n            break\n\n        input_sequence = input_tokenizer.texts_to_sequences([input_text])\n        input_sequence = pad_sequences(input_sequence, maxlen=max_encoder_seq_length, padding='post')\n        response = decode_sequence(input_sequence)\n        print(f\"Bot: {response}\")\n\nif __name__ == \"__main__\":\n    chat()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-06T05:38:41.574709Z","iopub.execute_input":"2024-10-06T05:38:41.575203Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Loaded 304446 conversations.\nCreated 304445 input-target pairs.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}