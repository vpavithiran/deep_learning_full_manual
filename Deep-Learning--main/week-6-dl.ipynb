{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8482,"sourceType":"datasetVersion","datasetId":5689},{"sourceId":163621,"sourceType":"datasetVersion","datasetId":73247},{"sourceId":2020792,"sourceType":"datasetVersion","datasetId":1209510},{"sourceId":2510329,"sourceType":"datasetVersion","datasetId":1520310},{"sourceId":9478143,"sourceType":"datasetVersion","datasetId":5764856},{"sourceId":2859008,"sourceType":"kernelVersion"},{"sourceId":55281833,"sourceType":"kernelVersion"}],"dockerImageVersionId":30775,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#a\n# Import necessary libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, accuracy_score\nimport re\nimport string\n\n# Step 1: Load the dataset\n# Replace 'path_to_dataset/reviews.csv' with the actual path to the IMDb dataset.\ndf = pd.read_csv('/kaggle/input/movie-review/labelled_full_dataset.csv')\n\n# Check the structure of the dataframe\nprint(df.head())\n\n# Step 2: Data Preprocessing\n# Clean text function\ndef clean_text(text):\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    # Convert to lowercase\n    text = text.lower()\n    # Remove digits\n    text = re.sub(r'\\d+', '', text)\n    return text\n\n# Apply cleaning to the 'review' column\ndf['cleaned_text'] = df['review'].apply(clean_text)\n\n# Step 3: Convert text data into numerical vectors\nvectorizer = TfidfVectorizer(max_features=5000)  # Use TF-IDF for vectorization\nX = vectorizer.fit_transform(df['cleaned_text'])\ny = df['label']  # Use the 'label' column for sentiment labels\n\n# Step 4: Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Step 5: Build and train the logistic regression model\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n# Step 6: Evaluate the model\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.2f}')\nprint(classification_report(y_test, y_pred))\n\n# Sample Input (Review Text)\nsample_review = \"This movie was amazing! I loved every minute of it.\"\nsample_cleaned = clean_text(sample_review)\nsample_vectorized = vectorizer.transform([sample_cleaned])\n\n# Predict sentiment for the sample input\nsample_prediction = model.predict(sample_vectorized)\nprint(f'Sample Review Sentiment: {\"Positive\" if sample_prediction[0] == 1 else \"Negative\"}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:15:33.111749Z","iopub.execute_input":"2024-09-25T12:15:33.112490Z","iopub.status.idle":"2024-09-25T12:15:49.447495Z","shell.execute_reply.started":"2024-09-25T12:15:33.112432Z","shell.execute_reply":"2024-09-25T12:15:49.446351Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"   label                                             review\n0      0  Once again Mr. Costner has dragged out a movie...\n1      0  This is an example of why the majority of acti...\n2      0  First of all I hate those moronic rappers, who...\n3      0  Not even the Beatles could write songs everyon...\n4      0  Brass pictures (movies is not a fitting word f...\nAccuracy: 0.89\n              precision    recall  f1-score   support\n\n           0       0.90      0.88      0.89      5022\n           1       0.88      0.90      0.89      4978\n\n    accuracy                           0.89     10000\n   macro avg       0.89      0.89      0.89     10000\nweighted avg       0.89      0.89      0.89     10000\n\nSample Review Sentiment: Positive\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Step 1: Load and preprocess the dataset\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/twitter-sentiment/Sentiment Analysis Dataset 2.csv', on_bad_lines='skip')\n\n# Check the structure of the dataframe\nprint(df.head())\n\n# Clean text function\ndef clean_text(text):\n    # Remove URLs, mentions, hashtags, and special characters\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'@\\w+', '', text)\n    text = re.sub(r'#', '', text)\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = text.lower()\n    return text\n\n# Apply cleaning to the 'SentimentText' column\ndf['cleaned_text'] = df['SentimentText'].apply(clean_text)\n\n# Convert sentiment labels to numerical format\ndf['Sentiment'] = df['Sentiment'].replace({0: 0, 2: 1, 4: 2})  # Adjust based on your labeling scheme\n\n# Step 2: Tokenize text and pad sequences\nmax_length = 100  # Maximum length of sequences\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(df['cleaned_text'])\nsequences = tokenizer.texts_to_sequences(df['cleaned_text'])\npadded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n\n# Step 3: Split data into training, validation, and testing sets\nX = padded_sequences\ny = df['Sentiment'].values\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # Split the temp set into validation and test sets\n\n# Step 4: Load pre-trained GloVe embeddings\nembeddings_index = {}\nglove_file = '/kaggle/input/glove-embeddings/glove.6B.100d.txt'  # Adjust the path to your GloVe file\nwith open(glove_file, 'r', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n# Create embedding matrix\nembedding_dim = 100\nembedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n\n# Step 5: Build LSTM model with GloVe embeddings\nmodel = Sequential()\nmodel.add(Embedding(input_dim=len(tokenizer.word_index) + 1, \n                    output_dim=embedding_dim, \n                    weights=[embedding_matrix], \n                    input_length=max_length, \n                    trainable=False))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(3, activation='softmax'))  # 3 classes (0, 1, 2)\n\n# Compile the model\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Step 6: Train the model with validation data\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3)\nhistory = model.fit(X_train, y_train, epochs=3, batch_size=64, \n                    validation_data=(X_val, y_val), callbacks=[early_stopping])\n\n# Step 7: Evaluate the model\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Test Accuracy: {accuracy:.2f}')\n\n# Sample Input (Tweet Text)\nsample_tweet = \"I really enjoyed the movie, it was fantastic!\"\nsample_cleaned = clean_text(sample_tweet)\nsample_sequence = tokenizer.texts_to_sequences([sample_cleaned])\nsample_padded = pad_sequences(sample_sequence, maxlen=max_length, padding='post')\n\n# Predict sentiment for the sample input\nsample_prediction = model.predict(sample_padded)\nprint(f'Sample Tweet Sentiment: {np.argmax(sample_prediction)}')  # Output the sentiment class\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:16:07.197753Z","iopub.execute_input":"2024-09-25T12:16:07.198550Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"   ItemID  Sentiment SentimentSource  \\\n0       1          0    Sentiment140   \n1       2          0    Sentiment140   \n2       3          1    Sentiment140   \n3       4          0    Sentiment140   \n4       5          0    Sentiment140   \n\n                                       SentimentText  \n0                       is so sad for my APL frie...  \n1                     I missed the New Moon trail...  \n2                            omg its already 7:30 :O  \n3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n4           i think mi bf is cheating on me!!!   ...  \n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n\u001b[1m19111/19733\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2:02\u001b[0m 197ms/step - accuracy: 0.4987 - loss: 0.6969","output_type":"stream"}]},{"cell_type":"code","source":"import os\nprint(os.listdir('/kaggle/input/'))","metadata":{"execution":{"iopub.status.busy":"2024-09-25T10:13:48.923145Z","iopub.execute_input":"2024-09-25T10:13:48.923631Z","iopub.status.idle":"2024-09-25T10:13:48.930518Z","shell.execute_reply.started":"2024-09-25T10:13:48.923590Z","shell.execute_reply":"2024-09-25T10:13:48.929168Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"['using-word-embeddings-for-sentiment-analysis', 'imdb-review', 'twitter-sentiment', 'movie-review', 'twitter-entity-sentiment-analysis', 'd', 'twitter-sentiment-analysis-using-tensorflow']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Step 1: Load and preprocess the dataset\n# Load the dataset \ndf = pd.read_csv('/kaggle/input/imdb-review/imdb_reviews.csv')  # Adjust to your actual dataset path\n\n# Display the first few rows of the dataset\nprint(df.head())\n\n# Convert sentiment labels to numerical format (0 for negative, 1 for positive)\ndf['sentiment'] = df['sentiment'].map({'neg': 0, 'pos': 1})\n\n# Clean text function (optional, based on dataset specifics)\ndef clean_text(text):\n    # Here, you can add any cleaning steps if necessary (e.g., removing special characters)\n    return text\n\n# Apply text cleaning\ndf['text'] = df['text'].apply(clean_text)\n\n# Step 2: Tokenize and encode reviews using BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nclass IMDBDataset(Dataset):\n    def __init__(self, reviews, labels):\n        self.reviews = reviews\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.reviews)\n\n    def __getitem__(self, idx):\n        review = self.reviews[idx]\n        label = self.labels[idx]\n\n        # Tokenize and encode the review\n        encoding = tokenizer.encode_plus(\n            review,\n            add_special_tokens=True,\n            max_length=256,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# Step 3: Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n\n# Create datasets\ntrain_dataset = IMDBDataset(X_train.to_numpy(), y_train.to_numpy())\ntest_dataset = IMDBDataset(X_test.to_numpy(), y_test.to_numpy())\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# Step 4: Load pre-trained BERT model and fine-tune for sentiment classification\n# Load tokenizer from a local directory where BERT files are stored\ntokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('/kaggle/input/bert-base-uncased', num_labels=2)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n# Set up the optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Train the model\nmodel.train()\nfor epoch in range(3):  # Adjust number of epochs as necessary\n    total_loss = 0\n    for batch in train_loader:\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n    print(f'Epoch {epoch + 1}/{3}, Loss: {total_loss / len(train_loader)}')\n\n# Step 5: Evaluate the model\nmodel.eval()\npredictions, true_labels = [], []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        preds = torch.argmax(logits, dim=1).cpu().numpy()\n\n        predictions.extend(preds)\n        true_labels.extend(batch['labels'].cpu().numpy())\n\n# Calculate accuracy and classification report\naccuracy = accuracy_score(true_labels, predictions)\nprint(f'Accuracy: {accuracy}')\nprint(classification_report(true_labels, predictions, target_names=['Negative', 'Positive']))\n\n# Sample Input\nsample_review = \"The movie was boring and uninteresting.\"\nencoded_sample = tokenizer.encode_plus(\n    sample_review,\n    add_special_tokens=True,\n    max_length=256,\n    return_token_type_ids=False,\n    padding='max_length',\n    truncation=True,\n    return_attention_mask=True,\n    return_tensors='pt',\n)\n\n# Make prediction on the sample input\nmodel.eval()\nwith torch.no_grad():\n    input_ids = encoded_sample['input_ids'].to(device)\n    attention_mask = encoded_sample['attention_mask'].to(device)\n    output = model(input_ids, attention_mask=attention_mask)\n    prediction = torch.argmax(output.logits, dim=1).cpu().numpy()\n\nprint(f'Sample Input: \"{sample_review}\"')\nprint(f'Expected Output: {\"Positive\" if prediction[0] == 1 else \"Negative\"}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-25T11:16:48.115977Z","iopub.status.idle":"2024-09-25T11:16:48.116440Z","shell.execute_reply.started":"2024-09-25T11:16:48.116225Z","shell.execute_reply":"2024-09-25T11:16:48.116248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}